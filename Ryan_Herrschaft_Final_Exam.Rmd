---
title: "Ryan Herrschaft Final Exam"
output: html_notebook
---

Preamble: 
This is Ryan Herrschaft's final examination submission.

```{r}
#Clean out environment
rm(list = ls())

#Session info
date()
sessionInfo()

library (tidyverse)
library (Hmisc)
library (dplyr)
library(stringr)
#just include library commands of the packages you installed, not ned to have the install.packages line

Nola_Crime <- read_csv("/Users/ryanherrschaft/Downloads/nola_crime_2018.csv")
```

Question 1: How many observations are in the datbase?
Question 2" How many variables are there? 
Question 3: How many unique item numbers are there in this database?
```{r}
#Question 1: How many observations are in the datbase?
#Question 2" How many variables are there? 
# Both of these first two are answerable by looking at the Environment window.

#Question 3: How many unique item numbers are there in this database?
describe(Nola_Crime$Item_Number)


```
Questions 1-3: There are 9970 observations and 24 variables. There are 9384 unique item numbers.


4.) Considering only unique item numbers as crimes, how many “aggravated” crimes occured in the Irish Channel Neighborhood?
```{r}



#I found a command called grepl, matches what you write in the parentheses with  a string of characters in any collumn. 

Aggravated_crime_Irish <- filter(Nola_Crime, GNOCDC_LAB == "IRISH CHANNEL" & grepl('AGGRAVATED', Signal_Description)) 

describe(Aggravated_crime_Irish$Item_Number)
```
There was 1 unique identifiers for an aggravated crime occuring in the Irish Channel.


5. What two neighborhoods have the largest numbers of crime reports (unique items)?
```{r}
# I am taking the number of unique crimes, grouped according to the neighborhood, and displaying them in descending order.
Crimes_Distinct <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE)) %>% arrange(desc(item_numbers))
                               
head(Crimes_Distinct)
```
The French Quarter and Little Woods have the largest number of unique crime reports.

6. (10 points) Assume that the each person in the population contributes exactly one year of time at risk, calculate the rate of crime (unique items) per 1,000 persons in each neighborhood in the dataset for 2018, show this in a table in your report?

```{r}
#This is the same variable item variable as above, I am adding in population and then creating the rate

Crime_Rate_by_district <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop, na_rm=FALSE)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop)))
  
head(Crime_Rate_by_district)
```

7. (15 points) What is the largest neighborhood by population in the dataset and what is its total population?
```{r}
#I am just using the same code as last time, but I am going to arrange by population.
#Since the distrcits are grouped by nieghborhood, the mean population is just the population by neighborhood. I  could of had the population appear wihtout taking the mean, but I had some trouble doing this. Considering the mean of population is the same as the population, this should be the same table.

Crime_Rate_by_district <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop))) %>% arrange(desc(pop))
  
head(Crime_Rate_by_district)
```
The largest neighborhood by population is Little Woods, with a population of 44,947.

8. (10 points) What is the population size and number of unique crimes in the neighborhood with the lowest rate of crime (unique items) per 1,000 person years?

```{r}
#This is the same code as before, only I am arranging by the Rate of Crime in ascending order.

Q_8_Crime_Rate_by_district <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop))) %>% arrange(Rate_of_Crime_per_1000)
  
head(Q_8_Crime_Rate_by_district)
```
Lakewood has the lowest rate of crime. There were 10 incidents of crime and the area has a population of 13,989. 

9. (10 points) Calculate the rate ratio for each neighborhood for all unique crimes treating LAKE WOOD as the reference (denominator)?
```{r}
#The rate of crime in Lakewood was .7148474

Q_9_Crime_Rate_by_district <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop))) %>% mutate(Rate_Ratio_Lakewood = ((Rate_of_Crime_per_1000/.7148474)))
                           
head(Q_9_Crime_Rate_by_district)                                                              
```


10.)Calculate the rate ratio for each neighborhood for all unique “aggravated crimes” treating FRENCH QUARTER as the reference (denominator)?
```{r}
#First I am going to find out the rate of aggravateed crimes for all districts. I am going to arrange them in decreasing order of rate of crime. 

Aggravated_crime <- Nola_Crime %>% filter(grepl('AGGRAVATED', Signal_Description)) %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop))) %>% arrange(desc(Rate_of_Crime_per_1000))

head(Aggravated_crime)
#With this, I see that the French Quarter has the highest crime rate, .98866333. I then take this value as the denominator for the variable Rate_Ratio_FQ

#Note: It seems that some districts had no aggravated crimes and were dropped from sample. This leaves me with 57 observations, rather than 73.

#The below statement gives me the table I want. 

Q_10_Rate_Aggravated_crime <- Nola_Crime %>% filter(grepl('AGGRAVATED', Signal_Description)) %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop))) %>% mutate(Rate_Ratio_FQ = ((Rate_of_Crime_per_1000/.98866333)))

head(Q_10_Rate_Aggravated_crime)
```

11.) What is the most common cause for a police report to be filed (captured in the “signal description” variable) and what is the most common charge that is filed?
```{r}

Reports<- Nola_Crime %>%  group_by(Signal_Description) %>% summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE))  %>% arrange(desc(item_numbers))

head(Reports)
#The most common cause for a police report is Domestic Disturbance.

Charges <- Nola_Crime %>% group_by(Charge_Code) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE))  %>% arrange(desc(item_numbers))

head(Charges)
#Charge_Description was the appropriate variable to use here, instead of Charge_code

  #I see that the Charge_Code 14 67 is the most commonly filed code.
```

12.)Calculate the rate of domestic disturbance (again based on “signal descrip- tion”) by neighborhood and show this as a table in your report.
```{r}
dom_disturb <- Nola_Crime %>%  filter(str_detect(Signal_Description,"DOMESTIC")) %>% filter(str_detect(Signal_Description, "DISTURBANCE")) %>% group_by(GNOCDC_LAB) %>% summarise (item_numbers = n_distinct(Item_Number, na_rm= FALSE), pop=mean(pop)) %>% mutate(Rate_of_Crime_per_1000=((item_numbers/pop)))
#question only asks for rate (including person time is not necessary)
head(dom_disturb)
```

13.) Fit a linear regression model to a summary dataset that you create which contains the rate of all crimes calculated by neighborhood as in previous calculation and includes the average age of the victims in that neighborhood as a predictor, include the summary of the regression results in your report by using the summary() function.
```{r}
#I had trouble creating a code that would have all of the infomraiton I need appear in one table. So, I just created two different tables and then merged them.

Crime_Rate_13 <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= TRUE), pop=mean(pop)) %>%  mutate(Rate_of_Crime_per_1000=((item_numbers*1000/pop)))

Mean_age_avg<- aggregate(Victim_Age ~ GNOCDC_LAB, Nola_Crime, mean)

data_for_lm<-merge(Crime_Rate_13, Mean_age_avg)
head(data_for_lm)
 
lm_item_age<-glm(item_numbers~Victim_Age, data=data_for_lm)
#instead of item_numbers as your outcome, it is crime rate
summary(lm_item_age)

```

14.) Assume that high numbers of police reports happen in places with adequate police responsiveness but that true rates of criminal acts are similar everywhere. If this is true than the absolute numbers of crimes depend only on population size but the number of reports depends on both population size and crime rates. If these hypothesis are true, which neighborhood might have the worst police responsiveness (i.e. Low rates of crime reporting - i.e. low numbers of signals/reports and high population)?
```{r}
#This question asks us to interpret the crime rate in a different way than we originaly did. Now, we are interpreting it such that low rates of crime with high populations suggest not less crime.

#The prompt asks us to assume that number of crimes only depends on population. Therefore, in this scenario, the Crime/Population should stay constant, as for every increase in population, there is a  proportional increase in crime.

#By arranging the values in ascending order of Crime/Population, we can see where we have cases of population increasing without proportional increases in crime. Thus, the values at the top of our table may be those neighborhoods in which there is low police responsiveness.

Crime_14 <- Nola_Crime %>% group_by(GNOCDC_LAB) %>%  summarise (item_numbers = n_distinct(Item_Number, na_rm= TRUE), pop=mean(pop)) %>% mutate(Police_Response=((item_numbers*1000/pop))) %>% arrange(Police_Response)

head(Crime_14)
```
Lakewood possibly has the worst police response, as it has the lowest ratio of crime to population.

15.)Plot a histogram of 10,000 realizations of a Poisson distributed random variable with λ = 4.7.
```{r}
random_data <- rpois(10000, 4.7)

hist(random_data)
```

16.)Using the “iris” dataset, write a for loop that calculates the mean Sepal length for each of the species of iris and prints the results.
```{r}
# I spent a ton of time on this and just couldnt get it to work. I am sorry, due to time constraints I am going to move on. I did succeed in writing a function that calculates the mean, but I could not figure out how to get the mean to be separated out by the species. 

data(iris)
attach(iris)
summarized_iris <- NA
for (Species in c("setos", "versicolor", "virginica"))

{
summarised_iris <-iris %>% summarise(mean=mean(Sepal.Length))
  }
#this just summarizes the mean sepal length but the question wants the length for each of the three species of irises
#length for each of the species of iris and prints the results. 
print(summarised_iris)

detach(iris)
```


17.) Using the “mtcars” dataset make a scatter plot of mpg vs. disp, include a regression line fit with a linear regression model or a loess smooth regression, make the color of the points different for each different number of cylinders in the engine.
```{r}
data(mtcars)
ggplot(mtcars, aes(x= mpg, y= disp, color=cyl)) +
  geom_point() + labs(title="Relationship between Miles per Gallon and Displacement", x= "Miles per gallon", y="Displacement", color="Cylinder") +  geom_smooth(method='lm', formula= y~x)
```

18.) Write a function to convert a vector of continuous data into its Z-scores (standard normal deviates). The formula for a Z-score is (x−μ)/σ where x is the vector of 
data, μ is the mean of x and σ is the standard deviation of x. Use the function to calculate the Z-scores for this vector {-4.89, -1.93, -1.11, 3.94, 0.46, -3.85, -0.20, 6.04, 9.36, 7.26, 4.88, 13.45, -2.93, 6.39, -16.22, -3.24, 6.86, 11.87, 1.81, -2.54}.

```{r}
data <- c(-4.89, -1.93, -1.11, 3.94, 0.46, -3.85, -0.20, 6.04, 9.36, 7.26, 4.88, 13.45, -2.93, 6.39, -16.22, -3.24, 6.86, 11.87, 1.81, -2.54)

z_scores<-(data-mean(data))/sd(data)

Q_18<-rbind(data,z_scores)

head(Q_18)
```
The values, along with their z-scores, are visible above.


This is the end of the final examination.

Thank you for the class,
Ryan Herrschaft.